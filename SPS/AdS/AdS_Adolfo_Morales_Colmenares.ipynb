{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXXnSuFX0XFdROSzlYsQwr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zerxe/CEIABDAT/blob/main/SPS/AdS/AdS_Adolfo_Morales_Colmenares.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Análisis de sentimiento usando Python\n",
        "\n",
        "Adolfo Morales Colmenares"
      ],
      "metadata": {
        "id": "khjiS6_M41Wc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Frases para el análisis de sentimiento"
      ],
      "metadata": {
        "id": "ekZuOaHC5L8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"The movie was so awesome\",\n",
        "    \"The food here tastes terrible\",\n",
        "    \"Her presentation was very informative.\",\n",
        "    \"This place has a bad vibe.\",\n",
        "    \"The book was a thrilling read.\",\n",
        "    \"I’m not impressed with the quality.\",\n",
        "    \"The staff was friendly and welcoming.\",\n",
        "    \"The product didn’t meet my expectations.\",\n",
        "    \"I enjoyed every moment of the trip.\",\n",
        "    \"The instructions were confusing and unclear.\",\n",
        "    \"The team did an excellent job.\",\n",
        "    \"I feel disappointed with my purchase.\",\n",
        "    \"The atmosphere was lively and exciting.\",\n",
        "    \"I found the movie to be boring.\",\n",
        "    \"The food was delicious and satisfying.\",\n",
        "    \"I can't recommend this restaurant.\",\n",
        "    \"The experience was unforgettable.\",\n",
        "    \"The app crashes too often.\",\n",
        "    \"The presentation was quite engaging and inspiring.\",\n",
        "    \"I was really frustrated with the delay.\"\n",
        "]"
      ],
      "metadata": {
        "id": "sVw-QQAo5S_s"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Blob"
      ],
      "metadata": {
        "id": "nbkRwiW15Cdd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install textblob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gxe10ifA5B0q",
        "outputId": "0af23b3d-d02e-40a2-98ac-b340c58cde19"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "#Determining the Polarity and Subjectivity\n",
        "for sentence in sentences:\n",
        "    print(\"Polarity of Text     '\", sentence, \"' is\", TextBlob(sentence).sentiment.polarity)\n",
        "    print(\"Subjectivity of Text '\", sentence, \"' is\", TextBlob(sentence).sentiment.subjectivity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vp8GzWv96Dsu",
        "outputId": "18370af5-64ac-4313-da47-27b32d1f8f52"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Polarity of Text     ' The movie was so awesome ' is 1.0\n",
            "Subjectivity of Text ' The movie was so awesome ' is 1.0\n",
            "Polarity of Text     ' The food here tastes terrible ' is -1.0\n",
            "Subjectivity of Text ' The food here tastes terrible ' is 1.0\n",
            "Polarity of Text     ' Her presentation was very informative. ' is 0.2\n",
            "Subjectivity of Text ' Her presentation was very informative. ' is 0.3\n",
            "Polarity of Text     ' This place has a bad vibe. ' is -0.6999999999999998\n",
            "Subjectivity of Text ' This place has a bad vibe. ' is 0.6666666666666666\n",
            "Polarity of Text     ' The book was a thrilling read. ' is 0.25\n",
            "Subjectivity of Text ' The book was a thrilling read. ' is 1.0\n",
            "Polarity of Text     ' I’m not impressed with the quality. ' is -0.5\n",
            "Subjectivity of Text ' I’m not impressed with the quality. ' is 1.0\n",
            "Polarity of Text     ' The staff was friendly and welcoming. ' is 0.375\n",
            "Subjectivity of Text ' The staff was friendly and welcoming. ' is 0.5\n",
            "Polarity of Text     ' The product didn’t meet my expectations. ' is 0.0\n",
            "Subjectivity of Text ' The product didn’t meet my expectations. ' is 0.0\n",
            "Polarity of Text     ' I enjoyed every moment of the trip. ' is 0.5\n",
            "Subjectivity of Text ' I enjoyed every moment of the trip. ' is 0.7\n",
            "Polarity of Text     ' The instructions were confusing and unclear. ' is -0.3\n",
            "Subjectivity of Text ' The instructions were confusing and unclear. ' is 0.4\n",
            "Polarity of Text     ' The team did an excellent job. ' is 1.0\n",
            "Subjectivity of Text ' The team did an excellent job. ' is 1.0\n",
            "Polarity of Text     ' I feel disappointed with my purchase. ' is -0.75\n",
            "Subjectivity of Text ' I feel disappointed with my purchase. ' is 0.75\n",
            "Polarity of Text     ' The atmosphere was lively and exciting. ' is 0.21818181818181817\n",
            "Subjectivity of Text ' The atmosphere was lively and exciting. ' is 0.65\n",
            "Polarity of Text     ' I found the movie to be boring. ' is -1.0\n",
            "Subjectivity of Text ' I found the movie to be boring. ' is 1.0\n",
            "Polarity of Text     ' The food was delicious and satisfying. ' is 0.75\n",
            "Subjectivity of Text ' The food was delicious and satisfying. ' is 1.0\n",
            "Polarity of Text     ' I can't recommend this restaurant. ' is 0.0\n",
            "Subjectivity of Text ' I can't recommend this restaurant. ' is 0.0\n",
            "Polarity of Text     ' The experience was unforgettable. ' is 0.8\n",
            "Subjectivity of Text ' The experience was unforgettable. ' is 1.0\n",
            "Polarity of Text     ' The app crashes too often. ' is 0.0\n",
            "Subjectivity of Text ' The app crashes too often. ' is 0.0\n",
            "Polarity of Text     ' The presentation was quite engaging and inspiring. ' is 0.45\n",
            "Subjectivity of Text ' The presentation was quite engaging and inspiring. ' is 0.85\n",
            "Polarity of Text     ' I was really frustrated with the delay. ' is -0.7\n",
            "Subjectivity of Text ' I was really frustrated with the delay. ' is 0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VADER"
      ],
      "metadata": {
        "id": "Ao8VkbRF8vwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install vaderSentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49uANIv58wx8",
        "outputId": "8d671686-71f5-4a41-97f7-ddd4cd3bdd4c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2024.8.30)\n",
            "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "sentiment = SentimentIntensityAnalyzer()\n",
        "\n",
        "for sentence in sentences:\n",
        "    print(\"Sentiment of Text '\", sentence, \"' is\", sentiment.polarity_scores(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tsiij-2-831l",
        "outputId": "4fedee53-34fd-4a9d-9119-d780e1e5b885"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment of Text ' The movie was so awesome ' is {'neg': 0.0, 'neu': 0.433, 'pos': 0.567, 'compound': 0.7384}\n",
            "Sentiment of Text ' The food here tastes terrible ' is {'neg': 0.437, 'neu': 0.563, 'pos': 0.0, 'compound': -0.4767}\n",
            "Sentiment of Text ' Her presentation was very informative. ' is {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "Sentiment of Text ' This place has a bad vibe. ' is {'neg': 0.412, 'neu': 0.588, 'pos': 0.0, 'compound': -0.5423}\n",
            "Sentiment of Text ' The book was a thrilling read. ' is {'neg': 0.0, 'neu': 0.617, 'pos': 0.383, 'compound': 0.4767}\n",
            "Sentiment of Text ' I’m not impressed with the quality. ' is {'neg': 0.338, 'neu': 0.662, 'pos': 0.0, 'compound': -0.3724}\n",
            "Sentiment of Text ' The staff was friendly and welcoming. ' is {'neg': 0.0, 'neu': 0.396, 'pos': 0.604, 'compound': 0.7269}\n",
            "Sentiment of Text ' The product didn’t meet my expectations. ' is {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "Sentiment of Text ' I enjoyed every moment of the trip. ' is {'neg': 0.0, 'neu': 0.645, 'pos': 0.355, 'compound': 0.5106}\n",
            "Sentiment of Text ' The instructions were confusing and unclear. ' is {'neg': 0.494, 'neu': 0.506, 'pos': 0.0, 'compound': -0.4404}\n",
            "Sentiment of Text ' The team did an excellent job. ' is {'neg': 0.0, 'neu': 0.575, 'pos': 0.425, 'compound': 0.5719}\n",
            "Sentiment of Text ' I feel disappointed with my purchase. ' is {'neg': 0.383, 'neu': 0.617, 'pos': 0.0, 'compound': -0.4767}\n",
            "Sentiment of Text ' The atmosphere was lively and exciting. ' is {'neg': 0.0, 'neu': 0.396, 'pos': 0.604, 'compound': 0.7269}\n",
            "Sentiment of Text ' I found the movie to be boring. ' is {'neg': 0.277, 'neu': 0.723, 'pos': 0.0, 'compound': -0.3182}\n",
            "Sentiment of Text ' The food was delicious and satisfying. ' is {'neg': 0.0, 'neu': 0.374, 'pos': 0.626, 'compound': 0.7717}\n",
            "Sentiment of Text ' I can't recommend this restaurant. ' is {'neg': 0.345, 'neu': 0.655, 'pos': 0.0, 'compound': -0.2755}\n",
            "Sentiment of Text ' The experience was unforgettable. ' is {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "Sentiment of Text ' The app crashes too often. ' is {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "Sentiment of Text ' The presentation was quite engaging and inspiring. ' is {'neg': 0.0, 'neu': 0.465, 'pos': 0.535, 'compound': 0.6962}\n",
            "Sentiment of Text ' I was really frustrated with the delay. ' is {'neg': 0.545, 'neu': 0.455, 'pos': 0.0, 'compound': -0.7178}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BOW (Bag of Words Vectorization-Based Models)"
      ],
      "metadata": {
        "id": "MdWRYEzA9fAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the Dataset\n",
        "import pandas as pd\n",
        "#data = pd.read_csv('Finance_data.csv')\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/zerxe/CEIABDAT/refs/heads/main/SPS/AdS/data.csv')\n",
        "#Pre-Prcoessing and Bag of Word Vectorization using Count Vectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
        "cv = CountVectorizer(stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
        "text_counts = cv.fit_transform(data['Sentence'])\n",
        "#Splitting the data into trainig and testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(text_counts, data['Sentiment'], test_size=0.25, random_state=5)\n",
        "#Training the model\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "MNB = MultinomialNB()\n",
        "MNB.fit(X_train, Y_train)\n",
        "#Caluclating the accuracy score of the model\n",
        "from sklearn import metrics\n",
        "predicted = MNB.predict(X_test)\n",
        "accuracy_score = metrics.accuracy_score(predicted, Y_test)\n",
        "print(\"Accuracuy Score: \",accuracy_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fcfS0c99jBf",
        "outputId": "abf3a36e-1f50-4256-bc74-ad3cab826aa8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracuy Score:  0.6851471594798083\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM-Based Models"
      ],
      "metadata": {
        "id": "DlpBXSeKAaFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing necessary libraries\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from textblob import Word\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "#Loading the dataset\n",
        "#data = pd.read_csv('Finance_data.csv')\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/zerxe/CEIABDAT/refs/heads/main/SPS/AdS/data.csv')\n",
        "#Pre-Processing the text\n",
        "def cleaning(df, stop_words):\n",
        "    df['Sentence'] = df['Sentence'].apply(lambda x: ' '.join(x.lower() for x in x.split()))\n",
        "    # Replacing the digits/numbers\n",
        "    df['Sentence'] = df['Sentence'].str.replace('d', '')\n",
        "    # Removing stop words\n",
        "    df['Sentence'] = df['Sentence'].apply(lambda x: ' '.join(x for x in x.split() if x not in stop_words))\n",
        "    # Lemmatization\n",
        "    df['Sentence'] = df['Sentence'].apply(lambda x: ' '.join([Word(x).lemmatize() for x in x.split()]))\n",
        "    return df\n",
        "stop_words = stopwords.words('english')\n",
        "data_cleaned = cleaning(data, stop_words)\n",
        "#Generating Embeddings using tokenizer\n",
        "tokenizer = Tokenizer(num_words=500, split=' ')\n",
        "tokenizer.fit_on_texts(data_cleaned['Sentence'].values)\n",
        "X = tokenizer.texts_to_sequences(data_cleaned['Sentence'].values)\n",
        "X = pad_sequences(X)\n",
        "\n",
        " # Convert Sentiment to numeric labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(data['Sentiment'].values)  # Usar 'Sentiment'\n",
        "y = pd.get_dummies(y).values  # Convertir a one-hot encoding\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "#Model Building\n",
        "model = Sequential()\n",
        "model.add(Embedding(500, 120, input_length = X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.4))\n",
        "model.add(LSTM(704, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(352, activation='leaky_relu'))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
        "print(model.summary())\n",
        "#Model Training\n",
        "model.fit(X_train, y_train, epochs = 20, batch_size=32, verbose =1)\n",
        "#Model Testing\n",
        "model.evaluate(X_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OUeBIm5aAisJ",
        "outputId": "bb7e96ec-ea69-462f-8bdf-6204043ad2e2"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ spatial_dropout1d_3                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "│ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)                   │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ spatial_dropout1d_3                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)                   │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Epoch 1/20\n",
            "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 1s/step - accuracy: 0.5503 - loss: 1.0104\n",
            "Epoch 2/20\n",
            "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 1s/step - accuracy: 0.6398 - loss: 0.8139\n",
            "Epoch 3/20\n",
            "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 1s/step - accuracy: 0.6710 - loss: 0.7448\n",
            "Epoch 4/20\n",
            "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 1s/step - accuracy: 0.6932 - loss: 0.6900\n",
            "Epoch 5/20\n",
            "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 995ms/step - accuracy: 0.6999 - loss: 0.6770\n",
            "Epoch 6/20\n",
            "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 984ms/step - accuracy: 0.7018 - loss: 0.6519\n",
            "Epoch 7/20\n",
            "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 944ms/step - accuracy: 0.7231 - loss: 0.5978\n",
            "Epoch 8/20\n",
            "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 947ms/step - accuracy: 0.7237 - loss: 0.5945\n",
            "Epoch 9/20\n",
            "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 942ms/step - accuracy: 0.7158 - loss: 0.6051\n",
            "Epoch 10/20\n",
            "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 945ms/step - accuracy: 0.7293 - loss: 0.5648\n",
            "Epoch 11/20\n",
            "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 986ms/step - accuracy: 0.7422 - loss: 0.5538\n",
            "Epoch 12/20\n",
            "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 949ms/step - accuracy: 0.7403 - loss: 0.5467\n",
            "Epoch 13/20\n",
            "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 932ms/step - accuracy: 0.7402 - loss: 0.5393\n",
            "Epoch 14/20\n",
            "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 926ms/step - accuracy: 0.7581 - loss: 0.5086\n",
            "Epoch 15/20\n",
            "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 918ms/step - accuracy: 0.7770 - loss: 0.4562\n",
            "Epoch 16/20\n",
            "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 897ms/step - accuracy: 0.6431 - loss: 0.8143\n",
            "Epoch 17/20\n",
            "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 915ms/step - accuracy: 0.7591 - loss: 0.5228\n",
            "Epoch 18/20\n",
            "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 915ms/step - accuracy: 0.7671 - loss: 0.4917\n",
            "Epoch 19/20\n",
            "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 913ms/step - accuracy: 0.7792 - loss: 0.4618\n",
            "Epoch 20/20\n",
            "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 904ms/step - accuracy: 0.7914 - loss: 0.4301\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 312ms/step - accuracy: 0.6563 - loss: 0.9207\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9600532650947571, 0.6501283049583435]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer-Based Models"
      ],
      "metadata": {
        "id": "9EOPnvJqoW-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Le__rbXvowQR",
        "outputId": "008266e7-8d87-4115-b9ec-bd0a22c5421c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from transformers import pipeline\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
        "#data = [\"It was the best of times.\", \"t was the worst of times.\"]\n",
        "sentiment_pipeline(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sD9V__pNoyoy",
        "outputId": "6ed36e24-debb-4a8a-8026-c0fe129c0886"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9998658895492554},\n",
              " {'label': 'NEGATIVE', 'score': 0.9986854195594788},\n",
              " {'label': 'POSITIVE', 'score': 0.9998227953910828},\n",
              " {'label': 'NEGATIVE', 'score': 0.9998014569282532},\n",
              " {'label': 'POSITIVE', 'score': 0.9998693466186523},\n",
              " {'label': 'NEGATIVE', 'score': 0.9991797804832458},\n",
              " {'label': 'POSITIVE', 'score': 0.9998666048049927},\n",
              " {'label': 'NEGATIVE', 'score': 0.9997861981391907},\n",
              " {'label': 'POSITIVE', 'score': 0.9998843669891357},\n",
              " {'label': 'NEGATIVE', 'score': 0.9993657469749451},\n",
              " {'label': 'POSITIVE', 'score': 0.9998359680175781},\n",
              " {'label': 'NEGATIVE', 'score': 0.9997860789299011},\n",
              " {'label': 'POSITIVE', 'score': 0.9998875856399536},\n",
              " {'label': 'NEGATIVE', 'score': 0.9998165965080261},\n",
              " {'label': 'POSITIVE', 'score': 0.9998881816864014},\n",
              " {'label': 'POSITIVE', 'score': 0.9931256771087646},\n",
              " {'label': 'POSITIVE', 'score': 0.9998482465744019},\n",
              " {'label': 'NEGATIVE', 'score': 0.9997400641441345},\n",
              " {'label': 'POSITIVE', 'score': 0.9998869895935059},\n",
              " {'label': 'NEGATIVE', 'score': 0.9995729327201843}]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    }
  ]
}